#!/usr/bin/env python3

# Blue File Reader with HCB and Extended Header Parsing
# This script reads and parses the HCB (Header Control Block) and
# extended header keywords from a Blue file format.

# Author: Don Marshall (with help from AI!)
# Date: October 28, 2025

import os
import json
import struct
import scipy.signal
import numpy as np
import matplotlib.pyplot as plt
from astropy.time import Time
from sigmf import SigMFFile # Assuming sigmf library is installed


# --- HCB Layout (fixed fields up to adjunct) ---
HCB_LAYOUT = [
    ("version",   0,   4,  "4s",   "Header version"),
    ("head_rep",  4,   4,  "4s",   "Header representation"),
    ("data_rep",  8,   4,  "4s",   "Data representation"),
    ("detached",  12,  4,  "i",    "Detached header"),
    ("protected", 16,  4,  "i",    "Protected from overwrite"),
    ("pipe",      20,  4,  "i",    "Pipe mode (N/A)"),
    ("ext_start", 24,  4,  "i",    "Extended header start (512-byte blocks)"),
    ("ext_size",  28,  4,  "i",    "Extended header size in bytes"),
    ("data_start",32,  8,  "d",    "Data start in bytes"),
    ("data_size", 40,  8,  "d",    "Data size in bytes"),
    ("type",      48,  4,  "i",    "File type code"),
    ("format",    52,  2,  "2s",   "Data format code"),
    ("flagmask",  54,  2,  "h",    "16-bit flagmask"),
    ("timecode",  56,  8,  "d",    "Time code field"),
    ("inlet",     64,  2,  "h",    "Inlet owner"),
    ("outlets",   66,  2,  "h",    "Number of outlets"),
    ("outmask",   68,  4,  "i",    "Outlet async mask"),
    ("pipeloc",   72,  4,  "i",    "Pipe location"),
    ("pipesize",  76,  4,  "i",    "Pipe size in bytes"),
    ("in_byte",   80,  8,  "d",    "Next input byte"),
    ("out_byte",  88,  8,  "d",    "Next out byte (cumulative)"),
    ("outbytes",  96,  64, "8d",   "Next out byte (each outlet)"),
    ("keylength", 160, 4,  "i",    "Length of keyword string"),
    ("keywords",  164, 92, "92s",  "User defined keyword string"),
    # Adjunct starts at 256
]

# --- Extended header type map ---
TYPE_MAP = {
    "B": (np.int8,    1),
    "I": (np.int16,   2),
    "L": (np.int32,   4),
    "X": (np.int64,   8),
    "F": (np.float32, 4),
    "D": (np.float64, 8),
    "A": (np.dtype("S1"), 1),
}

def read_hcb(path, endian="<"):
    """Read HCB fields and adjunct block."""
    HEADER_SIZE = 512
    hcb = {}
    with open(path, "rb") as f:
        data = f.read(HEADER_SIZE)

        # Fixed fields
        for name, offset, size, fmt, desc in HCB_LAYOUT:
            raw = data[offset:offset+size]
            val = struct.unpack(endian+fmt, raw)[0]
            if isinstance(val, bytes):
                val = val.decode("ascii", errors="replace").strip("\x00 ")
            hcb[name] = val

        # Adjunct parsing
        f.seek(256)
        if hcb["type"] == 1000 or 1001:
            hcb["adjunct"] = {
                "xstart": struct.unpack(f"{endian}d", f.read(8))[0],
                "xdelta": struct.unpack(f"{endian}d", f.read(8))[0],
                "xunits": struct.unpack(f"{endian}i", f.read(4))[0],
            }
        elif hcb["type"] == 2000:
            hcb["adjunct"] = {
                "xstart": struct.unpack(f"{endian}d", f.read(8))[0],
                "xdelta": struct.unpack(f"{endian}d", f.read(8))[0],
                "xunits": struct.unpack(f"{endian}i", f.read(4))[0],
                "subsize": struct.unpack(f"{endian}i", f.read(4))[0],
                "ystart": struct.unpack(f"{endian}d", f.read(8))[0],
                "ydelta": struct.unpack(f"{endian}d", f.read(8))[0],
                "yunits": struct.unpack(f"{endian}i", f.read(4))[0],
            }
        else:
            f.seek(256)
            hcb["adjunct_raw"] = f.read(256)

    return hcb

def parse_extended_header(file_path, hcb, endian="<"):
    """Parse extended header keyword records.
    Args:
        file_path: Path to BLUE file
        hcb: Header Control Block containing ext_size and ext_start
        endian: Endianness ('<' for little-endian, '>' for big-endian)
        
    Returns:
        List of dictionaries containing parsed records
    """
    if hcb["ext_size"] <= 0:
        return []
    entries = []
    with open(file_path, "rb") as f:
        f.seek(int(hcb["ext_start"]) * 512)
        bytesRemaining = int(hcb["ext_size"])
        while bytesRemaining > 0:
            lkey = struct.unpack(f"{endian}i", f.read(4))[0]
            lext = struct.unpack(f"{endian}h", f.read(2))[0]
            ltag = struct.unpack(f"{endian}b", f.read(1))[0]
            type_char = f.read(1).decode("ascii", errors="replace")

            dtype, bytesPerElement = TYPE_MAP.get(type_char, (np.dtype("S1"), 1))
            val_len = lkey - lext
            val_count = val_len // bytesPerElement if bytesPerElement else 0

            if type_char == "A":
                raw = f.read(val_len)
                value = raw.rstrip(b"\x00").decode("ascii", errors="replace")
            else:
                value = np.frombuffer(f.read(val_len), dtype=dtype, count=val_count)
                if value.size == 1:
                    value = value[0]
                else:
                    value = value.tolist()

            tag = f.read(ltag).decode("ascii", errors="replace") if ltag > 0 else ""

            total = 4+2+1+1+val_len+ltag
            pad = (8 - (total % 8)) % 8
            if pad: f.read(pad)

            entries.append({
                "tag": tag, "type": type_char, "value": value,
                "lkey": lkey, "lext": lext, "ltag": ltag
            })
            bytesRemaining -= lkey
    return entries


def parse_data_values(path, hcb, endian="<"):
    """Parse key HCB values that are used for further processing.
        Args:
        path: Path to Blue File
        hcb: Header Control Block dictionary
        endian: Endianness ('<' for little, '>' for big)
    Returns:
        numpy.ndarray: Parsed samples
    """

    HEADER_SIZE = 512
    SUPPORTED_TYPES = {'CI', 'CL', 'CF', 'SB', 'SI', 'SL', 'SX', 'SF', 'SD'}
    
    print("===== Parsing blue file data values =====")
    with open(path, "rb") as f:
        data = f.read(HEADER_SIZE)
        dtype = data[52:54].decode('utf-8') # eg 'CI', 'CF', 'SD'
        print('Data type: ', dtype)
        if dtype not in SUPPORTED_TYPES:
            raise ValueError(f"Unsupported data type: {dtype}")
        endianness = data[8:12].decode('utf-8') # better be 'EEEI'! we'll assume it is from this point on
        print('Endianness: ', endianness)
        if endianness != 'EEEI':
            raise ValueError(f"Unexpected endianness: {endianness}")    
        time_interval = np.frombuffer(data[264:272], dtype=np.float64)[0]
        sample_rate = 1/time_interval
        print('Sample rate: ', sample_rate/1e6, 'MHz')
        extended_header_data_size = int.from_bytes(data[28:32], byteorder='little')
        filesize = os.path.getsize(path)
        print('File size: ', filesize)

    # Complex data parsing

    # complex 16-bit integer  IQ data
    if dtype == 'CI':
      samples = np.fromfile(filename, dtype=np.int16, offset=HEADER_SIZE, count=(filesize-extended_header_data_size))
      samples = samples[::2] + 1j*samples[1::2] # convert to IQIQIQ...

    if dtype == 'CL':
      sample_size=np.dtype(np.complex32).itemsize  # Will be 4 bytes
      sample_count = (filesize - extended_header_data_size) // sample_size
      samples = np.fromfile(filename, dtype=np.int32, offset=HEADER_SIZE, count=(filesize-extended_header_data_size))
      samples = samples[::2] + 1j*samples[1::2] # convert to IQIQIQ...

    if dtype == 'CF':
      # Each complex sample is 8 bytes (2 x float32), so np.complex64 is appropriate
      # samples = np.fromfile(filename, dtype=np.complex64, offset=HEADER_SIZE, count=(filesize-extended_header_data_size))
      # No need to reassemble IQ â€” already complex
      # Each sample is 8 bytes (float64), so compute count
      sample_size=np.dtype(np.complex64).itemsize  # Will be 8 bytes
      sample_count = (filesize - extended_header_data_size) // sample_size
      samples = np.fromfile(filename, dtype=np.complex64, offset=HEADER_SIZE, count=sample_count)
 
    # ToDo - how to handle Scalar types properly? 
    # Reshape per mathlab port?
    """
            fmt_size_char = self.hcb["format"][0]
            fmt_type_char = self.hcb["format"][1]

            elementsPerSample = self.FormatSize(fmt_size_char)
            print('Elements Per Sample', elementsPerSample/1e6)
            elem_count_per_sample = (
                np.prod(elementsPerSample) if isinstance(elementsPerSample, (tuple, list)) else elementsPerSample
            )

            dtype_str, elem_bytes = self.FormatType(fmt_type_char)
            bytesPerSample = int(elem_bytes) * int(elem_count_per_sample)

            bytesRead = int(self.dataOffset - self.hcb["data_start"])
            bytesRemaining = int(self.hcb["data_size"] - bytesRead)

    """


    # Scalar data parsing
    if dtype == 'SB':
      sample_size=np.dtype(np.int8).itemsize 
      samples = np.fromfile(filename, dtype=np.int8, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # Scalar data parsing
    if dtype == 'SI':
      sample_size=np.dtype(np.int16).itemsize 
      samples = np.fromfile(filename, dtype=np.int16, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # Scalar data parsing
    if dtype == 'SL':
      sample_size=np.dtype(np.int32).itemsize 
      samples = np.fromfile(filename, dtype=np.int32, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # Scalar data parsing
    if dtype == 'SX':
      sample_size=np.dtype(np.int64).itemsize 
      samples = np.fromfile(filename, dtype=np.int64, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # Scalar data parsing
    if dtype == 'SF':
      sample_size=np.dtype(np.float32).itemsize 
      samples = np.fromfile(filename, dtype=np.float32, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # Scalar data parsing
    if dtype == 'SD':
      sample_size=np.dtype(np.float64).itemsize 
      samples = np.fromfile(filename, dtype=np.float64, offset=HEADER_SIZE, count=(filesize - extended_header_data_size) // sample_size)

    # ToDo - determine if required and whne to use normalization
    # Normalize samples to -1.0 to +1.0 range
    # samples = samples / 32767.0

    # Save out as SigMF IQ data file
    dest_path = filename.rsplit(".",1)[0]
    samples.astype(np.complex64).tofile(f"{dest_path}.sigmf-data")

    # Plot output for debugging
    debug_plot_signal(samples, sample_rate)
    input("Press Enter to continue...")  # Pauses until user presses Enter

    return samples

def blue_to_sigmf(hcb, ext_entries, data_path):
    """
    Build a SigMF metadata dict from parsed BLUE HCB + extended header.
    Args:
    hcb: Header Control Block dict from read_hcb()
    ext_entries: list of dicts from parse_extended_header()
    data_path: path to the .sigmf-data file
    Returns:
        dict: SigMF metadata structure
    Raises:
        ValueError: If required fields are missing or invalid
    """
    # Helper to look up extended header values by tag
    def get_tag(tag):
        for e in ext_entries:
            if e["tag"] == tag:
                return e["value"]
        return None

# S - Scalar
# C-  Complex
# V - Vector
# Q-  Quad - ToDo - add support for other types.

# B: 8-bit integer
# I: 16-bit integer
# L: 32-bit integer
# X: 64-bit integer
# F: 32-bit float
# D: 64-bit float

    # --- Global datatype object - little endian---
    datatype_map_le = {
        "SB": "ri8_le",
        "SI": "ri16_le",
        "SL": "ri32_le",
        "SX": "ri64_le",
        "SF": "rf32_le",
        "SD": "rf642_le",
        "CB": "ci8_le",
        "CI": "ci16_le",
        "CL": "ci32_le",
        "CX": "ci64_le",
        "CF": "cf32_le",
        "CD": "cf32_le",
    }

    # --- Global datatype object - big endian---
    datatype_map_be = {
        "SB": "ri8_be",
        "SI": "ri16_be",
        "SL": "ri32_be",
        "SX": "ri64_be",
        "SF": "rf32_be",
        "SD": "rf642_be",
        "CB": "ci8_be",
        "CI": "ci16_be",
        "CL": "ci32_be",
        "CX": "ci64_be",
        "CF": "cf32_be",
        "CD": "cf32_be",
    }

    # data_rep  : 'EEEI'  # Data endianess representation
    data_rep = hcb.get("data_rep")

    # data_format : For example 'CI'  or 'SD'  Data format code - real or complex, int or float
    data_format = hcb.get("format")
    
    if data_rep == "EEEI": # Big Endian
        data_map = datatype_map_be.get(data_format)
    
    elif data_rep == "IEEE": # Little Endian
        data_map = datatype_map_le.get(data_format)
    
    datatype = data_map if data_map is not None else "unknown"

    print(f"Determined SigMF datatype: {datatype}")
    
    # Sample rate: prefer adjunct.xdelta, else extended header SAMPLE_RATE
    if "adjunct" in hcb and "xdelta" in hcb["adjunct"]:
        sample_rate = 1.0 / hcb["adjunct"]["xdelta"]
    else:
        sr = get_tag("SAMPLE_RATE")
        sample_rate = float(sr) if sr is not None else None

    # For now define static values. Perhaps take as JSON input 
    hardware_description = "Blue File Conversion - Unknown Hardware"
    blue_author = "Blue File Conversion - Unknown Author"
    blue_licence = "Blue File Conversion - Unknown Licence"

    global_md = {
        "core:author": blue_author,
        "core:datatype": datatype,
        "core:description": hcb.get("keywords", ""),
        "core:hw": hardware_description,
        "core:core:license": blue_licence,
        "core:num_channels": int(hcb.get("outlets", 1)),
        "core:sample_rate": sample_rate,
        "core:version": "1.0.0",
    }

    # --- Captures array ---
    captures = [{
        "core:datetime": get_tag("TIME_EPOCH"),
        "core:frequency": float(get_tag("RF_FREQ") or 0.0),
        "core:sample_start": 0,
    }]

    # --- Annotations array ---
    annotations = []

    # Build one annotation dict with all HCB fields
    hcb_annotation = {}
 
    for name, _, _, _, desc in HCB_LAYOUT:
        # hcb_annotation[f"blue:hcb_{name}"] = hcb[name]
        value = hcb.get(name)             # safe access
        if value is None:
            continue                      # or set a default
        hcb_annotation[f"blue:hcb_{name}"] = value
    
    annotations.append(hcb_annotation)

    # Build one annotation dict with all adjunct fields
    adjunct_annotation = {}
    adjunct = hcb.get("adjunct", {})    
    for key, value in adjunct.items():
        adjunct_annotation[f"blue:adjunct_header_{key}"] = value   

    annotations.append(adjunct_annotation)     

    # Build one annotation dict with all of the extended header fields
    extended_header_annotation = {}

    for e in ext_entries:
        name = e.get("tag")
        if name is None:
           continue
        key = f"blue:extended_header_{name}"
        value = e.get("value")
        if hasattr(value, "item"):
            value = value.item()
        extended_header_annotation[key] = value
    annotations.append(extended_header_annotation)

    # --- Final SigMF object ---
    sigmf = {
        "global": global_md,
        "captures": captures,
        "annotations": annotations,
    }

    # Write .sigmf-meta file
    meta_path = os.path.splitext(data_path)[0] + ".sigmf-meta"
    with open(meta_path, "w") as f:
        json.dump(sigmf, f, indent=2)
    print(f"==== Wrote SigMF metadata to {meta_path} ====")

    return sigmf


def debug_plot_signal(samples, sample_rate):
    """Optional signal plot used for checking IQ signal decode."""

    # Plot signal for debugging
    Fs = sample_rate

# ToDo - fix plotting for complex data

    print(f"Sample type: {samples.dtype}")
    print(f"First few samples: {samples[:5]}")
    print(f"Min/Max real: {np.min(samples.real):.2f}/{np.max(samples.real):.2f}")
    print(f"Min/Max imag: {np.min(samples.imag):.2f}/{np.max(samples.imag):.2f}")

    # Normalize if needed based on data type?
    if samples.dtype in [np.int16, np.int32]:
        samples = samples / np.max(np.abs(samples))
    
    # Take a subset of samples if too many points
    max_samples = 1000
    if len(samples) > max_samples:
        step = len(samples) // max_samples
        subsetsamples = samples[::step]

    # IQ Plot
    plt.plot(subsetsamples.real, ".-", label="I")
    plt.plot(subsetsamples.imag, ".-", label="Q")
    plt.xlabel("Sample Index")
    plt.ylabel("Amplitude")
    plt.title("IQ Signal Components")
    plt.legend()
    plt.grid(True)
    plt.show()
    
    # Frequency domain plots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    n = len(samples)
    [frequencies, psd_db_hz] = calc_power_density_spectrum(Fs=Fs, n=n, iqarray=samples)

    # ax1.figure()
    ax1.plot(frequencies, psd_db_hz)
    ax1.set_title("Power Density spectrum")
    ax1.set_xlabel("Frequency [Hz]")
    ax1.set_ylabel("Power Density [dB/Hz]")
    ax1.grid()

   # ax2.specgram(data.iq_data)
    Pxx, freqs, bins, im = ax2.specgram(samples)
    ax2.set_title("Spectogram")
    ax2.set_xlabel("Time [s]")
    ax2.set_ylabel("Frequency [Hz]")
    fig.colorbar(im, ax=ax2).set_label("Power Density [dB/Hz]")
    plt.tight_layout()
    plt.show()

    return

def calc_power_density_spectrum(Fs: float, n: int, iqarray):
    """calculates the power density spectrum of an array of iq data.

    :param float Fs: sampling frequency
    :param int n: length of iq array
    :param _type_ iqarray: iq data in complex array
    :return _type_: frequency bins and logarithmic amplitudes

    """
    window = scipy.signal.windows.hamming(n)
    iq_array = iqarray * window
    if Fs is None:
        Fs = 1

    fft_sig = np.fft.fft(iq_array)
    fft_sig = np.fft.fftshift(fft_sig)
    frequencies = np.fft.fftfreq(n, 1 / Fs)
    frequencies = np.fft.fftshift(frequencies)

    psd = (np.abs(fft_sig) ** 2) / (Fs * n)
    psd_db_hz = 10 * np.log10(psd)

    return [frequencies, psd_db_hz]


def dump_blue_file(path):
    """ Main function to dump blue file contents."""

    print("==========================================")
    print("===== Starting blue file processing =====")
    print("==========================================")

    # ToDo - determine endian from head_rep field with error checking
    hcb = read_hcb(path, endian="<")  # Assuming little-endian
    
    print("=== Header Control Block (HCB) Fields ===")
    for name, _, _, _, desc in HCB_LAYOUT:
        print(f"{name:10s}: {hcb[name]!r}  # {desc}")

    print("\n=== Adjunct Header ===")
    print(hcb.get("adjunct", hcb.get("adjunct_raw")))

    # ToDo - determine endian from head_rep field with error checking
    # Get endian from data_rep
    # data_rep = hcb.get("data_rep")
    #     endian = "<" if data_rep == "IEEE" else ">"
    endian = "<"

    ext = parse_extended_header(path, hcb, endian)
    print("\n=== Extended Header Keywords ===")
    for e in ext:
        print(f"{e['tag']:20s}:{e['value']}")
    print(f"Total extended header entries: {len(ext)}")

    # Parse key data values    
    iq_data = parse_data_values(path, hcb, endian)

    # Call the SigMF conversion for metadata generation 
    blue_to_sigmf(hcb, ext, f"{path}.sigmf-data")

if __name__ == "__main__":
    """ Main calls dump blue file contents."""
    # ToDo - add command line args for filename
    filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\SceptretTestFile1.cdif'
    # filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\RustBlueTestFiles\pulse_cx.tmp' # or cdif
    # filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\RustBlueTestFiles\sin.tmp' 
    # filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\RustBlueTestFiles\penny.prm' 
    # filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\RustBlueTestFiles\keyword_test_file.tmp' # or cdif
    # filename = 'C:\Data1\Ham_Radio\SDR\SigMF-MIDAS-Blue-File-Conversion\PythonDevCode\RustBlueTestFiles\lots_of_keywords.tmp' # or cdif
    dump_blue_file(filename)
